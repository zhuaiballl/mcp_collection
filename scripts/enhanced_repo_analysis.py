import os
import json
import csv
import re
import argparse
import subprocess
from collections import defaultdict, Counter
from pathlib import Path
import xml.etree.ElementTree as ET
import tomli  # ç”¨äºè§£æCargo.toml
from typing import Dict, List, Set, Tuple, Optional

class EnhancedRepoAnalyzer:
    def __init__(self):
        # æ”¯æŒçš„è¯­è¨€å’Œå¯¹åº”çš„ä¾èµ–æ–‡ä»¶æ¨¡å¼
        # ä¿®æ”¹LANGUAGE_PATTERNSé…ç½®ï¼Œæ”¯æŒå¤šç§Pythonä¾èµ–æ–‡ä»¶æ ¼å¼
        self.LANGUAGE_PATTERNS = {
            'Python': {
                'files': [
                    {'file': 'requirements.txt', 'parser': self.parse_python_requirements},
                    {'file': 'pyproject.toml', 'parser': self.parse_pyproject_toml},
                    {'file': 'setup.py', 'parser': self.parse_setup_py},
                    {'file': 'Pipfile', 'parser': self.parse_pipfile},
                    {'file': 'poetry.lock', 'parser': self.parse_poetry_lock}
                ]
            },
            'JavaScript': {
                'files': [{'file': 'package.json', 'parser': self.parse_package_json}]
            },
            'Java': {
                'files': [{'file': 'pom.xml', 'parser': self.parse_pom_xml}]
            },
            'Go': {
                'files': [{'file': 'go.mod', 'parser': self.parse_go_mod}]
            },
            'Rust': {
                'files': [{'file': 'Cargo.toml', 'parser': self.parse_cargo_toml}]
            },
            'Ruby': {
                'files': [{'file': 'Gemfile', 'parser': self.parse_gemfile}]
            }
        }


        # è¯­è¨€æ–‡ä»¶æ‰©å±•åæ˜ å°„
        self.LANGUAGE_EXTENSIONS = {
            '.py': 'Python',
            '.js': 'JavaScript',
            '.jsx': 'JavaScript',
            '.ts': 'JavaScript',
            '.tsx': 'JavaScript',
            '.java': 'Java',
            '.go': 'Go',
            '.rs': 'Rust',
            '.rb': 'Ruby'
        }

        # æ ‡å‡†åº“æ˜ å°„ï¼Œç”¨äºè¿‡æ»¤éç¬¬ä¸‰æ–¹ä¾èµ–
        self.STANDARD_LIBRARIES = {
            'Python': set([
                'os', 'sys', 'json', 're', 'collections', 'datetime', 'pathlib',
                'math', 'random', 'time', 'io', 'csv', 'logging', 'argparse',
                'subprocess', 'tempfile', 'shutil', 'hashlib', 'itertools', 'functools',
                'threading', 'multiprocessing', 'socket', 'http', 'urllib', 'email'
            ]),
            'JavaScript': set([
                'fs', 'path', 'http', 'https', 'url', 'os', 'crypto', 'stream',
                'events', 'util', 'buffer', 'querystring', 'child_process'
            ]),
            'Java': set([
                'java.lang', 'java.util', 'java.io', 'java.net', 'java.nio',
                'java.time', 'java.text', 'java.math', 'java.security',
                'java.sql', 'java.awt', 'javax.swing'
            ]),
            'Go': set([
                'fmt', 'strings', 'strconv', 'time', 'encoding/json', 'os',
                'path/filepath', 'sync', 'io', 'net/http', 'reflect', 'errors'
            ]),
            'Rust': set([
                'std', 'core', 'alloc', 'collections', 'time', 'fs', 'io',
                'net', 'path', 'thread', 'sync'
            ]),
            'Ruby': set([
                'Kernel', 'Object', 'Module', 'Class', 'String', 'Array',
                'Hash', 'Integer', 'Float', 'Fixnum', 'Bignum', 'File', 'Dir',
                'IO', 'Time', 'Regexp', 'Exception', 'Enumerable'
            ])
        }

        # ä¾èµ–åˆ†ç±»æ˜ å°„ï¼Œç”¨äºå°†ä¾èµ–æ˜ å°„åˆ°æ›´é«˜å±‚æ¬¡çš„ç±»åˆ«
        self.DEPENDENCY_CATEGORIES = {
            'Webæ¡†æ¶': {
                'Python': ['flask', 'django', 'fastapi', 'bottle', 'tornado', 'pyramid'],
                'JavaScript': ['express', 'react', 'vue', 'angular', 'koa', 'nest'],
                'Java': ['spring', 'jakarta', 'quarkus', 'micronaut', 'play'],
                'Go': ['gin', 'echo', 'fiber', 'gorilla', 'beego'],
                'Rust': ['actix', 'rocket', 'warp', 'axum', 'tide'],
                'Ruby': ['rails', 'sinatra', 'rack', 'hanami', 'padrino']
            },
            'æ•°æ®åº“': {
                'Python': ['sqlalchemy', 'pymongo', 'django.db', 'psycopg2', 'mysql-connector', 'sqlite3'],
                'JavaScript': ['mongoose', 'sequelize', 'typeorm', 'prisma', 'knex'],
                'Java': ['hibernate', 'mybatis', 'jdbc', 'jpa', 'spring-data'],
                'Go': ['gorm', 'sqlx', 'go-sql-driver/mysql', 'lib/pq'],
                'Rust': ['diesel', 'sqlx', 'tokio-postgres', 'mongodb'],
                'Ruby': ['activerecord', 'mongoid', 'sequel']
            },
            'AI/æœºå™¨å­¦ä¹ ': {
                'Python': ['tensorflow', 'pytorch', 'scikit-learn', 'numpy', 'pandas', 'matplotlib', 'keras', 'langchain'],
                'JavaScript': ['tensorflow.js', 'brain.js', 'synaptic', 'ml5.js'],
                'Java': ['dl4j', 'weka', 'h2o', 'deeplearning4j'],
                'Go': ['gorgonia', 'goml'],
                'Rust': ['tract', 'linfa', 'ndarray'],
                'Ruby': ['sciruby', 'nmatrix']
            },
            'æµ‹è¯•å·¥å…·': {
                'Python': ['pytest', 'unittest', 'nose', 'mock', 'coverage'],
                'JavaScript': ['jest', 'mocha', 'chai', 'jasmine', 'cypress'],
                'Java': ['junit', 'testng', 'mockito', 'assertj'],
                'Go': ['testing', 'testify', 'gocheck'],
                'Rust': ['tokio-test', 'assert_cmd', 'proptest'],
                'Ruby': ['rspec', 'minitest', 'cucumber']
            },
            'APIå·¥å…·': {
                'Python': ['requests', 'aiohttp', 'httpx', 'flask-restful', 'fastapi'],
                'JavaScript': ['axios', 'superagent', 'fetch'],
                'Java': ['retrofit', 'okhttp', 'feign'],
                'Go': ['go-json-rest', 'resty', 'grequests'],
                'Rust': ['reqwest', 'hyper'],
                'Ruby': ['faraday', 'httparty', 'rest-client']
            },
            'å®‰å…¨å·¥å…·': {
                'Python': ['pycryptodome', 'bcrypt', 'cryptography', 'jwt'],
                'JavaScript': ['crypto-js', 'bcryptjs', 'jsonwebtoken'],
                'Java': ['bouncycastle', 'spring-security', 'jbcrypt'],
                'Go': ['crypto', 'bcrypt', 'jwt-go'],
                'Rust': ['ring', 'bcrypt', 'jsonwebtoken'],
                'Ruby': ['bcrypt', 'devise', 'omniauth']
            }
        }

        # éƒ¨ç½²æ–¹å¼æ£€æµ‹é…ç½®
        self.DEPLOYMENT_PATTERNS = {
            'Docker': {
                'files': ['Dockerfile', 'docker-compose.yml', 'docker-compose.yaml'],
                'confidence': 'high'
            },
            'Vercel': {
                'files': ['vercel.json'],
                'keywords': ['vercel'],
                'confidence': 'high'
            },
            'Railway': {
                'files': ['railway.json'],
                'keywords': ['railway'],
                'confidence': 'high'
            },
            'Heroku': {
                'files': ['Procfile', 'heroku.yml'],
                'keywords': ['heroku'],
                'confidence': 'high'
            },
            'AWS': {
                'keywords': ['aws', 'amazon web services', 'lambda', 's3', 'ec2'],
                'confidence': 'medium'
            },
            'Google Cloud': {
                'keywords': ['gcp', 'google cloud', 'cloud run', 'firebase'],
                'confidence': 'medium'
            },
            'GitHub Pages': {
                'files': ['.github/workflows/deploy.yml'],
                'keywords': ['github pages', 'gh-pages'],
                'confidence': 'medium'
            }
        }

        # æ ¸å¿ƒæ¡†æ¶æ£€æµ‹é…ç½®
        self.FRAMEWORK_DETECTION = {
            'Python': {
                'Flask': {'imports': ['from flask import Flask'], 'dependencies': ['flask']},
                'Django': {'imports': ['from django.db import', 'from django.http import'], 'dependencies': ['django']},
                'FastAPI': {'imports': ['from fastapi import FastAPI'], 'dependencies': ['fastapi']},
                'LangChain': {'imports': ['import langchain'], 'dependencies': ['langchain']}
            },
            'JavaScript': {
                'Express': {'imports': ['const express = require(\'express\')', 'import express from \'express\''], 'dependencies': ['express']},
                'React': {'imports': ['import React from \'react\''], 'dependencies': ['react']},
                'Vue': {'imports': ['import Vue from \'vue\''], 'dependencies': ['vue']},
                'NestJS': {'dependencies': ['@nestjs/core']}
            },
            'Go': {
                'Gin': {'dependencies': ['github.com/gin-gonic/gin']},
                'Echo': {'dependencies': ['github.com/labstack/echo/v4']}
            },
            'Rust': {
                'Actix': {'dependencies': ['actix-web']},
                'Rocket': {'dependencies': ['rocket']}
            }
        }

    def detect_language(self, repo_path: Path) -> str:
        """æ£€æµ‹ä»“åº“çš„ä¸»è¦ç¼–ç¨‹è¯­è¨€"""
        language_counts = defaultdict(int)
        # é€šè¿‡æ–‡ä»¶æ‰©å±•åæ£€æµ‹
        for root, _, files in os.walk(repo_path):
            for file in files:
                ext = Path(file).suffix.lower()
                language = self.LANGUAGE_EXTENSIONS.get(ext)
                if language:
                    language_counts[language] += 1
                
                # ä¹Ÿé€šè¿‡ä¾èµ–æ–‡ä»¶æ£€æµ‹ - ä¿®å¤è¿™é‡Œï¼
                if root == repo_path:
                    for lang, info in self.LANGUAGE_PATTERNS.items():
                        if 'files' in info:
                            for file_config in info['files']:
                                if file == file_config['file']:
                                    language_counts[lang] += 5  # ä¾èµ–æ–‡ä»¶æƒé‡æ›´é«˜
        
        if not language_counts:
            return 'Unknown'
        
        # è¿”å›æ–‡ä»¶æ•°æœ€å¤šçš„è¯­è¨€
        return max(language_counts.items(), key=lambda x: x[1])[0]

    # ä¿®å¤parse_pyproject_tomlæ–¹æ³•
    def parse_pyproject_toml(self, file_path: Path) -> Set[str]:
        """è§£æpyproject.tomlæ–‡ä»¶ä¸­çš„ä¾èµ–"""
        dependencies = set()
        try:
            # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼æ‰“å¼€æ–‡ä»¶
            with open(file_path, 'rb') as f:
                data = tomli.load(f)
                # è§£æpoetryä¾èµ–
                if 'tool' in data and 'poetry' in data['tool'] and 'dependencies' in data['tool']['poetry']:
                    for lib_name in data['tool']['poetry']['dependencies']:
                        # è·³è¿‡pythonè‡ªèº«
                        if lib_name.lower() != 'python' and lib_name.lower() not in self.STANDARD_LIBRARIES.get('Python', set()):
                            dependencies.add(lib_name.lower())
                # è§£æsetuptoolsä¾èµ–
                if 'project' in data and 'dependencies' in data['project']:
                    for dep in data['project']['dependencies']:
                        # å¤„ç†å½¢å¦‚ "package>=1.0.0" çš„æ ¼å¼
                        match = re.match(r'([a-zA-Z0-9_\-\.]+)', dep)
                        if match:
                            lib_name = match.group(1).lower()
                            if lib_name not in self.STANDARD_LIBRARIES.get('Python', set()):
                                dependencies.add(lib_name)
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
        return dependencies
    
    # ä¿®å¤parse_python_requirementsæ–¹æ³•ï¼Œå¢åŠ ç¼–ç å¤„ç†
    def parse_python_requirements(self, file_path: Path) -> Set[str]:
        """è§£æPythonçš„requirements.txtæ–‡ä»¶"""
        dependencies = set()
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç 
            encodings = ['utf-8', 'latin-1', 'cp1252']
            content = None
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.readlines()
                    break  # æˆåŠŸè¯»å–åè·³å‡ºå¾ªç¯
                except UnicodeDecodeError:
                    continue
                
            if content:
                for line in content:
                    line = line.strip()
                    if line and not line.startswith('#') and line != '\n':
                        # å¤„ç†ç±»ä¼¼ package==1.0.0 çš„æ ¼å¼
                        match = re.match(r'([a-zA-Z0-9_\-\.]+)', line)
                        if match:
                            lib_name = match.group(1).lower()
                            # è¿‡æ»¤æ ‡å‡†åº“
                            if lib_name not in self.STANDARD_LIBRARIES.get('Python', set()):
                                dependencies.add(lib_name)
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
        return dependencies
    


    def parse_setup_py(self, file_path: Path) -> Set[str]:
        """è§£æsetup.pyæ–‡ä»¶ä¸­çš„ä¾èµ–"""
        dependencies = set()
        try:
            with open(file_path, 'r') as f:
                content = f.read()
                # å°è¯•æå–install_requireså‚æ•°
                install_requires_match = re.search(r'install_requires\s*=\s*\[(.*?)\]', content, re.DOTALL)
                if install_requires_match:
                    deps_str = install_requires_match.group(1)
                    # æå–æ‰€æœ‰ä¾èµ–åç§°
                    for match in re.finditer(r'["\']([a-zA-Z0-9_\-\.]+)', deps_str):
                        lib_name = match.group(1).lower()
                        if lib_name not in self.STANDARD_LIBRARIES.get('Python', set()):
                            dependencies.add(lib_name)
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
        return dependencies

    def parse_pipfile(self, file_path: Path) -> Set[str]:
        """è§£æPipfileæ–‡ä»¶ä¸­çš„ä¾èµ–"""
        dependencies = set()
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç 
            encodings = ['utf-8', 'latin-1', 'cp1252']
            data = None
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                        # å¤„ç†å¯èƒ½çš„JSONè§£æé—®é¢˜
                        if content.strip():
                            data = json.loads(content)
                    break  # æˆåŠŸè¯»å–åè·³å‡ºå¾ªç¯
                except (UnicodeDecodeError, json.JSONDecodeError):
                    continue
            
            if data is not None:
                # è§£ædefaultä¾èµ–
                if 'default' in data:
                    for lib_name in data['default']:
                        if lib_name.lower() not in self.STANDARD_LIBRARIES.get('Python', set()):
                            dependencies.add(lib_name.lower())
                # è§£ædevelopä¾èµ–
                if 'develop' in data:
                    for lib_name in data['develop']:
                        if lib_name.lower() not in self.STANDARD_LIBRARIES.get('Python', set()):
                            dependencies.add(lib_name.lower())
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
        return dependencies

    def parse_poetry_lock(self, file_path: Path) -> Set[str]:
        """è§£æpoetry.lockæ–‡ä»¶ä¸­çš„ä¾èµ–"""
        dependencies = set()
        try:
            with open(file_path, 'r') as f:
                content = f.read()
                # æå–æ‰€æœ‰packageåç§°
                for match in re.finditer(r'name\s*=\s*["\']([a-zA-Z0-9_\-\.]+)["\']', content):
                    lib_name = match.group(1).lower()
                    if lib_name not in self.STANDARD_LIBRARIES.get('Python', set()):
                        dependencies.add(lib_name)
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
        return dependencies

    def parse_package_json(self, file_path: Path) -> Set[str]:
        """è§£æJavaScriptçš„package.jsonæ–‡ä»¶"""
        dependencies = set()
        try:
            # å¦‚æœæ˜¯node_modulesç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œé‡‡å–æ›´é«˜æ•ˆçš„å¤„ç†æ–¹å¼
            if 'node_modules' in str(file_path).split(os.sep):
                # å¯¹äºnode_modulesä¸­çš„æ–‡ä»¶ï¼Œå¯ä»¥é‡‡ç”¨æ›´ç®€å•å¿«é€Ÿçš„è§£ææ–¹å¼
                # å°è¯•ä½¿ç”¨å¿«é€Ÿçš„äºŒè¿›åˆ¶è¯»å–
                content = None
                try:
                    with open(file_path, 'rb') as f:
                        content = f.read().decode('utf-8', errors='replace')
                except:
                    # å¦‚æœè¯»å–å¤±è´¥ï¼Œç›´æ¥è¿”å›ç©ºé›†åˆï¼Œé¿å…åœ¨node_modulesä¸ŠèŠ±è´¹å¤ªå¤šæ—¶é—´
                    return dependencies
            else:
                # énode_modulesç›®å½•ï¼Œä½¿ç”¨å®Œæ•´çš„ç¼–ç å¤„ç†
                content = None
                for encoding in ['utf-8', 'utf-16', 'latin-1']:
                    try:
                        with open(file_path, 'rb') as f:
                            content = f.read().decode(encoding)
                        break
                    except UnicodeDecodeError:
                        continue
                
                if content is None:
                    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨chardetå°è¯•æ£€æµ‹ç¼–ç 
                    try:
                        import chardet
                        with open(file_path, 'rb') as f:
                            raw_data = f.read()
                            result = chardet.detect(raw_data)
                            encoding = result['encoding'] or 'utf-8'
                            content = raw_data.decode(encoding, errors='replace')
                    except ImportError:
                        # å¦‚æœæ²¡æœ‰chardetåº“ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç å¹¶æ›¿æ¢é”™è¯¯
                        with open(file_path, 'rb') as f:
                            content = f.read().decode('utf-8', errors='replace')
            
            # ä¿å­˜åŸå§‹å†…å®¹ç”¨äºæœ€åçš„å¤‡é€‰è§£æ
            original_content = content
            
            # å°è¯•ä¿®å¤å¸¸è§çš„JSONè¯­æ³•é—®é¢˜
            # 1. ä¿®å¤å¤šä½™çš„é€—å·
            content = re.sub(r',\s*}', '}', content)
            content = re.sub(r',\s*\]', ']', content)
            
            # 2. ä¿®å¤å•å¼•å·
            content = re.sub(r"'([^']+)'", r'"\1"', content)
            
            # 3. ç§»é™¤è¡Œæ³¨é‡Š
            content = re.sub(r'//.*$', '', content, flags=re.MULTILINE)
            
            # 4. ç§»é™¤å¤šè¡Œæ³¨é‡Š
            content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
            
            # 5. å°è¯•è§£æä¿®å¤åçš„JSON
            try:
                data = json.loads(content)
            except json.JSONDecodeError:
                # å¦‚æœåŸºæœ¬ä¿®å¤å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ›´å®½æ¾çš„è§£æåº“
                try:
                    import json5
                    data = json5.loads(content)
                except (ImportError, Exception):
                    # å¦‚æœæ²¡æœ‰json5åº“æˆ–è€…ä»è§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¢å¼ºçš„æ­£åˆ™è¡¨è¾¾å¼ä½œä¸ºæœ€åçš„å¤‡é€‰
                    try:
                        # å¢å¼ºç‰ˆæ­£åˆ™è¡¨è¾¾å¼ï¼Œæ›´çµæ´»åœ°åŒ¹é…dependencieså’ŒdevDependencies
                        # å¤„ç†ä¸åŒçš„ç©ºæ ¼å’Œæ¢è¡Œæƒ…å†µ
                        dep_pattern = r'"dependencies"\s*:\s*\{([\s\S]*?)\}'
                        dev_dep_pattern = r'"devDependencies"\s*:\s*\{([\s\S]*?)\}'
                        
                        # æå–ä¾èµ–é¡¹å—
                        all_deps_content = ''
                        for pattern in [dep_pattern, dev_dep_pattern]:
                            match = re.search(pattern, original_content, re.DOTALL)
                            if match:
                                all_deps_content += match.group(1)
                        
                        # ä»ä¾èµ–å†…å®¹ä¸­æå–åŒ…åï¼Œä½¿ç”¨æ›´å¥å£®çš„æ­£åˆ™è¡¨è¾¾å¼
                        # åŒ¹é…å„ç§åˆæ³•çš„npmåŒ…åæ ¼å¼ï¼ŒåŒ…æ‹¬@scope/packageå½¢å¼
                        lib_pattern = r'"(@?[a-zA-Z0-9\-_\.]+(?:/[a-zA-Z0-9\-_\.]+)?)"\s*:'
                        lib_matches = re.findall(lib_pattern, all_deps_content)
                        
                        for lib_name in lib_matches:
                            if lib_name.lower() not in self.STANDARD_LIBRARIES.get('JavaScript', set()):
                                dependencies.add(lib_name.lower())
                        
                        # å¦‚æœæ‰¾åˆ°ä¾èµ–ï¼Œè¿”å›ç»“æœï¼Œä¸è¾“å‡ºé”™è¯¯
                        if dependencies:
                            return dependencies
                        
                        # ä½œä¸ºæœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼Œå°è¯•åŒ¹é…ä»»ä½•çœ‹èµ·æ¥åƒåŒ…åçš„å­—ç¬¦ä¸²
                        # è¿™ç§æ–¹æ³•å¯èƒ½ä¼šæœ‰ä¸€äº›è¯¯æŠ¥ï¼Œä½†æ€»æ¯”ä»€ä¹ˆéƒ½ä¸è¿”å›è¦å¥½
                        fallback_pattern = r'"(@?[a-zA-Z0-9\-_\.]+(?:/[a-zA-Z0-9\-_\.]+)?)"'
                        fallback_matches = re.findall(fallback_pattern, original_content)
                        
                        for lib_name in fallback_matches:
                            # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯åŒ…åçš„å­—ç¬¦ä¸²
                            if len(lib_name) > 2 and lib_name.lower() not in ['name', 'version', 'description', 'main', 'scripts', 'author', 'license', 'keywords', 'repository', 'homepage', 'bugs']:
                                if lib_name.lower() not in self.STANDARD_LIBRARIES.get('JavaScript', set()):
                                    dependencies.add(lib_name.lower())
                        
                        # å¦‚æœé€šè¿‡fallbackæ–¹æ³•æ‰¾åˆ°ä¾èµ–ï¼Œé™é»˜è¿”å›
                        if dependencies:
                            return dependencies
                        
                        # åªæœ‰åœ¨æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ä¸”æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä¾èµ–æ—¶æ‰è¾“å‡ºé”™è¯¯ä¿¡æ¯
                        if 'node_modules' not in str(file_path).split(os.sep):
                            print(f"Error parsing {file_path}: Invalid JSON format, even with enhanced regex fallback")
                    except Exception as e:
                        if 'node_modules' not in str(file_path).split(os.sep):
                            print(f"Error parsing {file_path}: {str(e)[:100]}...")
                    return dependencies
            
            # ä»æˆåŠŸè§£æçš„JSONä¸­æå–ä¾èµ–
            if 'dependencies' in data:
                for lib_name in data['dependencies']:
                    if lib_name.lower() not in self.STANDARD_LIBRARIES.get('JavaScript', set()):
                        dependencies.add(lib_name.lower())
            if 'devDependencies' in data:
                for lib_name in data['devDependencies']:
                    if lib_name.lower() not in self.STANDARD_LIBRARIES.get('JavaScript', set()):
                        dependencies.add(lib_name.lower())
        except Exception as e:
            if 'node_modules' not in str(file_path).split(os.sep):
                print(f"Error parsing {file_path}: {str(e)[:100]}...")  # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦
        return dependencies

    def parse_pom_xml(self, file_path: Path) -> Set[str]:
        """è§£æJavaçš„pom.xmlæ–‡ä»¶"""
        dependencies = set()
        try:
            import xml.etree.ElementTree as ET
            tree = ET.parse(file_path)
            root = tree.getroot()
            
            # å®šä¹‰å‘½åç©ºé—´æ˜ å°„
            namespaces = {
                'maven': 'http://maven.apache.org/POM/4.0.0'
            }
            
            # æŸ¥æ‰¾æ‰€æœ‰ä¾èµ–é¡¹
            for dependency in root.findall('.//maven:dependency', namespaces):
                # è·å–groupIdå’ŒartifactId
                group_id = dependency.find('maven:groupId', namespaces)
                artifact_id = dependency.find('maven:artifactId', namespaces)
                
                if group_id is not None and artifact_id is not None:
                    # ç»„åˆgroupIdå’ŒartifactIdä»¥è·å¾—å®Œæ•´æ ‡è¯†ç¬¦
                    full_id = f"{group_id.text}.{artifact_id.text}"
                    if full_id.lower() not in self.STANDARD_LIBRARIES.get('Java', set()):
                        dependencies.add(full_id.lower())
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
        return dependencies

    def parse_go_mod(self, file_path: Path) -> Set[str]:
        """è§£æGoçš„go.modæ–‡ä»¶"""
        dependencies = set()
        try:
            with open(file_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    # åŒ¹é… require è¡Œ
                    if line.startswith('require'):
                        parts = line.split()
                        if len(parts) >= 2:
                            # è·³è¿‡ require å…³é”®å­—
                            if parts[1] != '(':
                                dep = self._extract_meaningful_go_dependency(parts[1])
                                if dep and dep.lower() not in self.STANDARD_LIBRARIES.get('Go', set()):
                                    dependencies.add(dep.lower())
                        continue
                    # å¤„ç†å¤šè¡Œ require ä¸­çš„ä¾èµ–
                    if line and not line.startswith('//') and line != ')':
                        parts = line.split()
                        if parts:
                            dep = self._extract_meaningful_go_dependency(parts[0])
                            if dep and dep.lower() not in self.STANDARD_LIBRARIES.get('Go', set()):
                                dependencies.add(dep.lower())
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
        return dependencies
    
    def _extract_meaningful_go_dependency(self, full_path: str) -> str:
        """ä»å®Œæ•´çš„Goä¾èµ–è·¯å¾„ä¸­æå–æœ‰æ„ä¹‰çš„åº“å"""
        try:
            # åˆ†å‰²è·¯å¾„
            parts = full_path.split('/')
            
            # å¦‚æœè·¯å¾„å¤ªçŸ­ï¼Œè¿”å›åŸå§‹çš„æœ€åä¸€éƒ¨åˆ†
            if len(parts) <= 2:
                return parts[-1].split('@')[0]  # ç§»é™¤ç‰ˆæœ¬ä¿¡æ¯
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰ˆæœ¬å·åç¼€ (å¦‚ /v2, /v3)
            main_part = parts[-2] if len(parts) > 2 and parts[-1].startswith('v') and parts[-1][1:].isdigit() else parts[-1]
            
            # ç§»é™¤ç‰ˆæœ¬ä¿¡æ¯
            main_part = main_part.split('@')[0]
            
            # å¯¹äºå¸¸è§çš„é€šç”¨ç»„ä»¶åï¼Œå°è¯•è·å–æ›´å…·ä½“çš„éƒ¨åˆ†
            common_generic_terms = {'sdk', 'api', 'client', 'server', 'core', 'common', 'utils', 'tools'}
            if main_part.lower() in common_generic_terms and len(parts) > 2:
                # å°è¯•ç»„åˆå‰ä¸€éƒ¨åˆ†å’Œå½“å‰éƒ¨åˆ†
                return f"{parts[-2].split('@')[0]}_{main_part}" if not (parts[-2].startswith('v') and parts[-2][1:].isdigit()) else main_part
            
            return main_part
        except:
            # å¦‚æœå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
            return full_path.split('/')[-1].split('@')[0]

    def parse_cargo_toml(self, file_path: Path) -> Set[str]:
        """è§£æRustçš„Cargo.tomlæ–‡ä»¶"""
        dependencies = set()
        try:
            with open(file_path, 'rb') as f:
                data = tomli.load(f)
                # è·å–ä¾èµ–
                if 'dependencies' in data:
                    for lib_name in data['dependencies']:
                        if lib_name.lower() not in self.STANDARD_LIBRARIES.get('Rust', set()):
                            dependencies.add(lib_name.lower())
                # è·å–å¼€å‘ä¾èµ–
                if 'dev-dependencies' in data:
                    for lib_name in data['dev-dependencies']:
                        if lib_name.lower() not in self.STANDARD_LIBRARIES.get('Rust', set()):
                            dependencies.add(lib_name.lower())
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
        return dependencies

    def parse_gemfile(self, file_path: Path) -> Set[str]:
        """è§£æRubyçš„Gemfileæ–‡ä»¶"""
        dependencies = set()
        try:
            with open(file_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    # åŒ¹é… gem 'name' æˆ– gem "name"
                    match = re.match(r'gem\s+[\'"]([a-zA-Z0-9_\-]+)[\'"]', line)
                    if match:
                        lib_name = match.group(1).lower()
                        if lib_name not in self.STANDARD_LIBRARIES.get('Ruby', set()):
                            dependencies.add(lib_name)
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
        return dependencies

    def categorize_dependency(self, lib_name: str, language: str) -> List[str]:
        """å°†ä¾èµ–æ˜ å°„åˆ°ç±»åˆ«"""
        categories = []
        for category, lang_libs in self.DEPENDENCY_CATEGORIES.items():
            if language in lang_libs:
                for keyword in lang_libs[language]:
                    if keyword in lib_name:
                        categories.append(category)
                        break
        return categories if categories else ['å…¶ä»–']

    def detect_deployment_methods(self, repo_path: Path) -> Dict[str, str]:
        """æ£€æµ‹ä»“åº“çš„éƒ¨ç½²æ–¹å¼"""
        deployment_methods = {}
        
        # 1. æ£€æŸ¥é…ç½®æ–‡ä»¶
        for method, config in self.DEPLOYMENT_PATTERNS.items():
            if 'files' in config:
                for file_name in config['files']:
                    if (repo_path / file_name).exists():
                        deployment_methods[method] = config['confidence']
                        break
        
        # 2. æœç´¢READMEå’Œæ–‡æ¡£æ–‡ä»¶ä¸­çš„å…³é”®è¯
        readme_files = []
        for file_name in ['README.md', 'README', 'README.rst', 'docs/deployment.md']:
            file_path = repo_path / file_name
            if file_path.exists():
                readme_files.append(file_path)
        
        # 3. æœç´¢GitHub Actionsé…ç½®
        github_actions_dir = repo_path / '.github' / 'workflows'
        if github_actions_dir.exists():
            for file_name in os.listdir(github_actions_dir):
                if file_name.endswith('.yml') or file_name.endswith('.yaml'):
                    readme_files.append(github_actions_dir / file_name)
        
        # æœç´¢å…³é”®è¯
        for method, config in self.DEPLOYMENT_PATTERNS.items():
            if method in deployment_methods:  # å·²é€šè¿‡æ–‡ä»¶æ£€æµ‹ç¡®è®¤
                continue
                
            if 'keywords' in config:
                for file_path in readme_files:
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read().lower()
                            if any(keyword.lower() in content for keyword in config['keywords']):
                                deployment_methods[method] = config['confidence']
                                break
                    except Exception:
                        continue
        
        return deployment_methods

    def detect_frameworks(self, repo_path: Path, language: str, dependencies: Set[str]) -> Dict[str, str]:
        """æ£€æµ‹ä»“åº“ä½¿ç”¨çš„æ ¸å¿ƒæ¡†æ¶"""
        frameworks = {}
        
        # 1. é€šè¿‡ä¾èµ–æ£€æµ‹
        if language in self.FRAMEWORK_DETECTION:
            for framework, config in self.FRAMEWORK_DETECTION[language].items():
                if 'dependencies' in config:
                    for dep in config['dependencies']:
                        # å¯¹äºGoå’Œå…¶ä»–è¯­è¨€ï¼Œéœ€è¦å¤„ç†å®Œæ•´çš„åŒ…å
                        if language == 'Go':
                            # æ£€æŸ¥ä¾èµ–ä¸­æ˜¯å¦åŒ…å«å®Œæ•´çš„åŒ…å
                            for repo_dep in dependencies:
                                if dep.split('/')[-1] == repo_dep:
                                    frameworks[framework] = 'dependency'
                                    break
                        elif dep in dependencies:
                            frameworks[framework] = 'dependency'
                            break
        
        # 2. é€šè¿‡ä»£ç å¯¼å…¥è¯­å¥æ£€æµ‹
        if language in self.FRAMEWORK_DETECTION:
            # æŸ¥æ‰¾å¯èƒ½çš„å…¥å£æ–‡ä»¶
            entry_files = []
            if language == 'Python':
                entry_files = ['main.py', 'app.py', '__init__.py']
            elif language in ['JavaScript', 'TypeScript']:
                entry_files = ['index.js', 'server.js', 'app.js']
            elif language == 'Go':
                entry_files = ['main.go']
            
            # æœç´¢æ ¹ç›®å½•ä¸‹çš„æ–‡ä»¶
            for file_name in os.listdir(repo_path):
                if file_name in entry_files:
                    entry_files = [repo_path / file_name]
                    break
            
            # æœç´¢ä»£ç æ–‡ä»¶ä¸­çš„å¯¼å…¥è¯­å¥
            for framework, config in self.FRAMEWORK_DETECTION[language].items():
                if framework in frameworks:  # å·²é€šè¿‡ä¾èµ–æ£€æµ‹ç¡®è®¤
                    continue
                    
                if 'imports' in config:
                    for file_path in entry_files:
                        try:
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                content = f.read()
                                if any(imp_statement in content for imp_statement in config['imports']):
                                    frameworks[framework] = 'code'
                                    break
                        except Exception:
                            continue
        
        return frameworks

    def analyze_repository(self, repo_path: Path) -> Dict:
        """åˆ†æå•ä¸ªä»“åº“"""
        result = {
            'name': repo_path.name,
            'path': str(repo_path),
            'language': 'Unknown',
            'dependencies': [],
            'category_dependencies': defaultdict(int),
            'deployment_methods': {},
            'frameworks': {}
        }
        
        try:
            # æ£€æµ‹ä¸»è¦è¯­è¨€
            result['language'] = self.detect_language(repo_path)
            
            # è§£æä¾èµ–
            lang_info = self.LANGUAGE_PATTERNS.get(result['language'])
            if lang_info and 'files' in lang_info:
                for file_config in lang_info['files']:
                    # æ£€æŸ¥æ ¹ç›®å½•
                    dep_file = repo_path / file_config['file']
                    if dep_file.exists():
                        try:
                            parser_func = file_config['parser']
                            deps = parser_func(dep_file)
                            result['dependencies'].extend(list(deps))
                        except Exception as e:
                            print(f"è§£ææ–‡ä»¶ {dep_file} æ—¶å‡ºé”™: {str(e) or repr(e)}")
                    
                    # å°è¯•åœ¨å­ç›®å½•ä¸­æŸ¥æ‰¾ä¾èµ–æ–‡ä»¶
                    for root, _, files in os.walk(repo_path):
                        if file_config['file'] in files and root != repo_path:
                            dep_file = Path(root) / file_config['file']
                            try:
                                parser_func = file_config['parser']
                                sub_deps = parser_func(dep_file)
                                result['dependencies'].extend(list(sub_deps))
                            except Exception as e:
                                print(f"è§£ææ–‡ä»¶ {dep_file} æ—¶å‡ºé”™: {str(e) or repr(e)}")
            
            # å»é‡ä¾èµ–åˆ—è¡¨
            result['dependencies'] = list(set(result['dependencies']))
            
            # æ˜ å°„åˆ°ä¾èµ–ç±»åˆ«
            for dep in result['dependencies']:
                categories = self.categorize_dependency(dep, result['language'])
                for category in categories:
                    result['category_dependencies'][category] += 1
            
            # æ£€æµ‹éƒ¨ç½²æ–¹å¼
            result['deployment_methods'] = self.detect_deployment_methods(repo_path)
            
            # æ£€æµ‹ä½¿ç”¨çš„æ¡†æ¶
            result['frameworks'] = self.detect_frameworks(repo_path, result['language'], result['dependencies'])
            
        except Exception as e:
            # æ•è·æ‰€æœ‰å…¶ä»–å¼‚å¸¸å¹¶æä¾›è¯¦ç»†ä¿¡æ¯
            print(f"åˆ†æä»“åº“ {repo_path.name} æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e) or repr(e)}")
            # å³ä½¿å‡ºé”™ä¹Ÿè¿”å›å·²æ”¶é›†çš„ç»“æœ
        
        return result

    def run_analysis(self, repos_dir: str, output_dir: str, min_count: int = 1):
        """è¿è¡Œå®Œæ•´çš„åˆ†æå¹¶è¾“å‡ºç»“æœ"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ†ææ‰€æœ‰ä»“åº“
        all_results = []
        total_repos = 0
        error_count = 0
        
        print(f"å¼€å§‹åˆ†æä»“åº“ï¼Œç›®å½•: {repos_dir}")
        
        # è·å–ä»“åº“åˆ—è¡¨
        repo_paths = []
        for item in os.listdir(repos_dir):
            item_path = os.path.join(repos_dir, item)
            if os.path.isdir(item_path):
                repo_paths.append(Path(item_path))
        
        total_repos = len(repo_paths)
        print(f"å‘ç° {total_repos} ä¸ªä»“åº“")
        
        # å¤„ç†æ¯ä¸ªä»“åº“
        for i, repo in enumerate(repo_paths, 1):
            print(f'åˆ†æä»“åº“ {i}/{total_repos}: {repo.name}')
            try:
                result = self.analyze_repository(repo)
                all_results.append(result)
            except Exception as e:
                error_count += 1
                print(f'åˆ†æ {repo.name} æ—¶å‡ºé”™: {e}')
        
        # ç»Ÿè®¡ç»“æœ
        self._generate_statistics(all_results, output_dir, total_repos, min_count)
        
        print(f'\nåˆ†æå®Œæˆ!')
        print(f'- æ€»ä»“åº“æ•°: {total_repos}')
        print(f'- æˆåŠŸåˆ†æ: {len(all_results)}')
        print(f'- åˆ†æé”™è¯¯: {error_count}')
        print(f'- ç»“æœå·²ä¿å­˜åˆ° {output_dir}')

    def _generate_statistics(self, all_results: List[Dict], output_dir: str, total_repos: int, min_count: int):
        """ç”Ÿæˆå¹¶ä¿å­˜ç»Ÿè®¡ç»“æœ"""
        # ç»Ÿè®¡è¯­è¨€åˆ†å¸ƒ
        language_stats = defaultdict(int)
        for result in all_results:
            language_stats[result['language']] += 1
        
        # ç»Ÿè®¡åº“ä½¿ç”¨é¢‘ç‡
        library_stats = defaultdict(int)
        category_stats = defaultdict(int)
        lang_library_stats = defaultdict(Counter)
        
        # ç»Ÿè®¡éƒ¨ç½²æ–¹å¼
        deployment_stats = defaultdict(int)
        deployment_confidence_stats = defaultdict(lambda: defaultdict(int))
        
        # ç»Ÿè®¡æ¡†æ¶ä½¿ç”¨æƒ…å†µ
        framework_stats = defaultdict(int)
        lang_framework_stats = defaultdict(Counter)
        
        for result in all_results:
            # ç»Ÿè®¡åº“
            for dep in result['dependencies']:
                library_stats[dep] += 1
                lang_library_stats[result['language']][dep] += 1
            for category, count in result['category_dependencies'].items():
                category_stats[category] += count
            
            # ç»Ÿè®¡éƒ¨ç½²æ–¹å¼
            for method, confidence in result['deployment_methods'].items():
                deployment_stats[method] += 1
                deployment_confidence_stats[method][confidence] += 1
            
            # ç»Ÿè®¡æ¡†æ¶
            for framework in result['frameworks']:
                framework_stats[framework] += 1
                lang_framework_stats[result['language']][framework] += 1
        
        # è¿‡æ»¤ä½é¢‘æ¬¡çš„åº“
        filtered_library_stats = {k: v for k, v in library_stats.items() if v >= min_count}
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSON
        final_results = {
            'total_repositories': total_repos,
            'analyzed_repositories': len(all_results),
            'language_distribution': dict(language_stats),
            'library_usage': dict(filtered_library_stats),
            'category_usage': dict(category_stats),
            'deployment_methods': dict(deployment_stats),
            'deployment_confidence': {k: dict(v) for k, v in deployment_confidence_stats.items()},
            'framework_usage': dict(framework_stats),
            'detailed_results': all_results
        }
        
        output_json = Path(output_dir) / 'enhanced_analysis_detail.json'
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜åˆ°CSVï¼ˆé€‚åˆGraphPad Prismä½¿ç”¨çš„æ ¼å¼ï¼‰
        self._save_to_csv(filtered_library_stats, output_dir, 'library_statistics.csv', total_repos)
        self._save_to_csv(category_stats, output_dir, 'category_statistics.csv', total_repos)
        self._save_to_csv(deployment_stats, output_dir, 'deployment_statistics.csv', total_repos)
        self._save_to_csv(framework_stats, output_dir, 'framework_statistics.csv', total_repos)
        
        # æŒ‰è¯­è¨€ä¿å­˜åº“ä½¿ç”¨ç»Ÿè®¡
        for lang, stats in lang_library_stats.items():
            lang_output = Path(output_dir) / f'library_statistics_{lang.lower()}.csv'
            # è¿‡æ»¤è¯¥è¯­è¨€çš„ä½é¢‘æ¬¡åº“
            filtered_lang_stats = {k: v for k, v in stats.items() if v >= min_count}
            self._save_to_csv(filtered_lang_stats, output_dir, f'library_statistics_{lang.lower()}.csv', 
                            language_stats.get(lang, 1))
            
        # æŒ‰è¯­è¨€ä¿å­˜æ¡†æ¶ç»Ÿè®¡
        for lang, stats in lang_framework_stats.items():
            self._save_to_csv(dict(stats), output_dir, f'framework_statistics_{lang.lower()}.csv', 
                            language_stats.get(lang, 1))

    def _save_to_csv(self, stats: Dict[str, int], output_dir: str, filename: str, total_count: int):
        """å°†ç»Ÿè®¡ç»“æœä¿å­˜ä¸ºCSVæ ¼å¼"""
        output_csv = Path(output_dir) / filename
        
        with open(output_csv, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            # CSVå¤´éƒ¨ï¼Œé€‚åˆGraphPad Prismä½¿ç”¨
            writer.writerow(['åç§°', 'æ•°é‡', 'ç™¾åˆ†æ¯”'])
            
            # æŒ‰ä½¿ç”¨æ¬¡æ•°æ’åº
            for name, count in sorted(stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_count) * 100 if total_count > 0 else 0
                writer.writerow([name, count, f'{percentage:.2f}'])

    def clone_repos_from_json(self, json_path: str, output_dir: str, github_token: str):
        """ä»JSONæ–‡ä»¶ä¸­å…‹éš†ä»“åº“"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        
        # è¯»å–JSONæ–‡ä»¶
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"è¯»å–JSONæ–‡ä»¶å¤±è´¥: {e}")
            return
        
        # æå–å¹¶å¤„ç†GitHub URL
        url_info = []
        for item in data:
            if 'github_url' in item:
                url = item['github_url']
                # å¤„ç†ä¸åŒæ ¼å¼çš„GitHub URL
                if isinstance(url, str) and url:
                    # ç®€åŒ–çš„URLå¤„ç†
                    if 'github.com' in url:
                        # æå–ç”¨æˆ·åå’Œä»“åº“åçš„ç®€å•å®ç°
                        parts = url.strip('/').split('/')
                        if len(parts) >= 2:
                            # æ‰¾åˆ°ç”¨æˆ·åå’Œä»“åº“åçš„ç´¢å¼•
                            user_idx = parts.index('github.com') + 1 if 'github.com' in parts else None
                            if user_idx and user_idx + 1 < len(parts):
                                user = parts[user_idx]
                                repo = parts[user_idx + 1]
                                # ç§»é™¤.gitåç¼€
                                if repo.endswith('.git'):
                                    repo = repo[:-4]
                                url_info.append((f'https://github.com/{user}/{repo}', user, repo))
        
        # å»é‡
        unique_url_info = list({url: (url, user, repo) for url, user, repo in url_info}.values())
        print(f"\nğŸ“Š GitHubä»“åº“ç»Ÿè®¡:")
        print(f"   - å»é‡åæ€»å…±æœ‰ {len(unique_url_info)} ä¸ªå”¯ä¸€GitHubä»“åº“")
        
        # å…‹éš†ä»“åº“
        print(f"\nğŸš€ å¼€å§‹å…‹éš† {len(unique_url_info)} ä¸ªä»“åº“...")
        failed_urls = []
        repo_counter = {}  # ç”¨äºè·Ÿè¸ªé‡å¤çš„ä»“åº“å
        
        for url, user, repo in unique_url_info:
            failed = self._clone_repo(url, output_dir, user, repo, repo_counter, headers)
            if failed:
                failed_urls.append(failed)
        
        # å¤„ç†å¤±è´¥çš„å…‹éš†
        if failed_urls:
            with open(Path(output_dir) / "clone_failed.txt", "w", encoding="utf-8") as f:
                f.write("\n".join(failed_urls))
            print(f"\nâš ï¸ {len(failed_urls)} ä¸ªä»“åº“å…‹éš†å¤±è´¥ï¼Œè¯¦è§ clone_failed.txt")
        else:
            print("\nğŸ‰ æ‰€æœ‰ä»“åº“å…‹éš†æˆåŠŸ")

    def _clone_repo(self, url: str, output_dir: str, user: str, repo: str, repo_counter: Dict[str, int], headers: Dict[str, str]) -> Optional[str]:
        """ä½¿ç”¨GitHub APIå…‹éš†ä»“åº“åˆ°æŒ‡å®šç›®å½•ï¼Œå¤„ç†é‡å¤ä»“åº“å"""
        # åŸºç¡€æ–‡ä»¶å¤¹åç§°
        base_folder_name = f"{user}_{repo}"
        
        # å¤„ç†é‡å¤ä»“åº“å
        counter = repo_counter.get(base_folder_name, 0)
        folder_name = base_folder_name if counter == 0 else f"{base_folder_name}_{counter}"
        repo_counter[base_folder_name] = counter + 1
        
        dest = Path(output_dir) / folder_name

        if dest.exists():
            print(f"å·²å­˜åœ¨: {dest}")
            return None

        # æ£€æŸ¥ä»“åº“æ˜¯å¦å­˜åœ¨
        if not self._check_repo_exists(url, headers):
            return url

        print(f"\nå…‹éš†: {dest}")
        
        # æ„å»ºå¸¦è®¤è¯çš„URL
        github_token = headers['Authorization'].split(' ')[1]
        auth_url = url.replace('https://', f'https://{github_token}@')
        
        # æ‰§è¡Œå…‹éš†å‘½ä»¤
        process = subprocess.Popen(
            ["git", "clone", auth_url, str(dest)],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )

        for line in process.stdout:
            print(f"   [{folder_name}] {line.strip()}")
        retcode = process.wait()
        if retcode != 0:
            print(f"âŒ å…‹éš†å¤±è´¥: {dest}")
            return url
        else:
            print(f"âœ… å…‹éš†æˆåŠŸ: {dest}")
            return None

    def _check_repo_exists(self, url: str, headers: Dict[str, str]) -> bool:
        """æ£€æŸ¥ä»“åº“æ˜¯å¦å­˜åœ¨å¹¶å¯è®¿é—®"""
        api_url = url.replace('https://github.com', 'https://api.github.com/repos')
        try:
            response = requests.get(api_url, headers=headers, timeout=10)
            return response.status_code == 200
        except Exception:
            return False

# ä¸ºäº†æ”¯æŒç‹¬ç«‹è¿è¡Œå…‹éš†åŠŸèƒ½ï¼Œæ·»åŠ requestså¯¼å…¥æ£€æŸ¥
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False


def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆMCPä»“åº“åˆ†æå·¥å…·')
    subparsers = parser.add_subparsers(dest='command', help='é€‰æ‹©è¦æ‰§è¡Œçš„å‘½ä»¤')
    
    # å…‹éš†å‘½ä»¤
    clone_parser = subparsers.add_parser('clone', help='ä»JSONæ–‡ä»¶å…‹éš†ä»“åº“')
    clone_parser.add_argument('json_path', help='åŒ…å«github_urlçš„JSONæ–‡ä»¶è·¯å¾„')
    clone_parser.add_argument('output_dir', help='å…‹éš†ä»“åº“çš„è¾“å‡ºç›®å½•')
    clone_parser.add_argument('--token', help='GitHubä¸ªäººè®¿é—®ä»¤ç‰Œï¼Œå¦‚æœæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡GITHUB_TOKENè·å–')
    
    # åˆ†æå‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†æä»“åº“')
    analyze_parser.add_argument('--repo-dir', type=str, required=True, 
                              help='åŒ…å«ä»“åº“çš„ç›®å½•è·¯å¾„')
    analyze_parser.add_argument('--output-dir', type=str, default='analysis', 
                              help='è¾“å‡ºç»“æœçš„ç›®å½•è·¯å¾„')
    analyze_parser.add_argument('--min-count', type=int, default=1, 
                              help='æœ€å°ç»Ÿè®¡æ¬¡æ•°ï¼Œä½äºæ­¤å€¼çš„åº“å°†è¢«è¿‡æ»¤')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œå‘½ä»¤
    analyzer = EnhancedRepoAnalyzer()
    
    if args.command == 'clone':
        # æ£€æŸ¥requestsåº“æ˜¯å¦å¯ç”¨
        if not HAS_REQUESTS:
            print("é”™è¯¯ï¼šç¼ºå°‘requestsåº“ï¼Œè¯·è¿è¡Œ 'pip install requests' å®‰è£…")
            return
        
        # è·å–GitHubä»¤ç‰Œ
        github_token = args.token or os.environ.get('GITHUB_TOKEN')
        if not github_token:
            print("é”™è¯¯ï¼šæœªæ‰¾åˆ°GitHubä»¤ç‰Œï¼Œè¯·é€šè¿‡--tokenå‚æ•°æä¾›æˆ–è®¾ç½®GITHUB_TOKENç¯å¢ƒå˜é‡")
            return
        
        analyzer.clone_repos_from_json(args.json_path, args.output_dir, github_token)
    
    elif args.command == 'analyze':
        # éªŒè¯ä»“åº“ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.repo_dir):
            print(f"é”™è¯¯ï¼šä»“åº“ç›®å½•ä¸å­˜åœ¨: {args.repo_dir}")
            return
        
        analyzer.run_analysis(args.repo_dir, args.output_dir, args.min_count)
    
    else:
        parser.print_help()

if __name__ == '__main__':
    main()

